{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "WYC9jXnTRvW3"
   },
   "source": [
    "# Работа с текстом\n",
    "\n",
    "В этом домашнем задании вам предстоит поработать с текстовыми данными и научиться находить спам сообщения!"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 186,
   "metadata": {
    "id": "gmljLzJDRvW4"
   },
   "outputs": [],
   "source": [
    "import inspect\n",
    "import math\n",
    "import random\n",
    "import re\n",
    "from collections import Counter, defaultdict\n",
    "from string import punctuation\n",
    "\n",
    "import numpy as np\n",
    "from nltk import SnowballStemmer, download\n",
    "from nltk.corpus import stopwords\n",
    "from sklearn.linear_model import LogisticRegression\n",
    "from sklearn.metrics import classification_report, accuracy_score, f1_score\n",
    "from sklearn.model_selection import train_test_split"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 187,
   "metadata": {
    "id": "nllPeX1xACLr"
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[nltk_data] Downloading package stopwords to /home/wrdx/nltk_data...\n",
      "[nltk_data]   Package stopwords is already up-to-date!\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "True"
      ]
     },
     "execution_count": 187,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "download(\"stopwords\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 188,
   "metadata": {
    "id": "XTU13-rOACLr",
    "pycharm": {
     "name": "#%%\n"
    }
   },
   "outputs": [],
   "source": [
    "def set_seed(seed=42):\n",
    "    np.random.seed(seed)\n",
    "    random.seed(seed)\n",
    "\n",
    "\n",
    "# Этой функцией будут помечены все места, которые необходимо дозаполнить\n",
    "# Это могут быть как целые функции, так и отдельные части внутри них\n",
    "# Всегда можно воспользоваться интроспекцией и найти места использования этой функции :)\n",
    "def todo():\n",
    "    stack = inspect.stack()\n",
    "    caller_frame = stack[1]\n",
    "    function_name = caller_frame.function\n",
    "    line_number = caller_frame.lineno\n",
    "    raise NotImplementedError(f\"TODO at {function_name}, line {line_number}\")\n",
    "\n",
    "\n",
    "SEED = 0xC0FFEE\n",
    "set_seed(SEED)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 189,
   "metadata": {
    "id": "VRJVvs51RvW4"
   },
   "outputs": [],
   "source": [
    "def read_dataset(filename):\n",
    "    x, y = [], []\n",
    "    with open(filename, encoding=\"utf-8\") as file:\n",
    "        for line in file:\n",
    "            cl, sms = re.split(r\"^(ham|spam)[\\t\\s]+(.*)$\", line)[1:3]\n",
    "            x.append(sms)\n",
    "            y.append(cl)\n",
    "    return x, y"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 190,
   "metadata": {
    "id": "9YKoYXwsRvW5"
   },
   "outputs": [],
   "source": [
    "X, y = read_dataset(\"spam.txt\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 191,
   "metadata": {
    "id": "GCmIbwv6RvW5"
   },
   "outputs": [],
   "source": [
    "X_train, X_test, y_train, y_test = train_test_split(X, y, train_size=0.9, random_state=SEED, stratify=y)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 192,
   "metadata": {
    "id": "Isg1F2ClACLt",
    "pycharm": {
     "name": "#%%\n"
    }
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "ham: Two fundamentals of cool life: \"Walk, like you are the KING\"...! OR \"Walk like you Dont care,whoever is the KING\"!... Gud nyt\n",
      "ham: Haha... Where got so fast lose weight, thk muz go 4 a month den got effect... Gee,later we go aust put bk e weight.\n",
      "ham: I wish things were different. I wonder when i will be able to show you how much i value you. Pls continue the brisk walks no drugs without askin me please and find things to laugh about. I love you dearly.\n",
      "ham: Tmr then ü brin lar... Aiya later i come n c lar... Mayb ü neva set properly ü got da help sheet wif ü...\n",
      "ham: For many things its an antibiotic and it can be used for chest abdomen and gynae infections even bone infections.\n"
     ]
    }
   ],
   "source": [
    "for x_, y_ in zip(X_train[:5], y_train[:5]):\n",
    "    print(f\"{y_}: {x_}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 193,
   "metadata": {
    "id": "kX5UHxOiACLu"
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "Counter({'ham': 4344, 'spam': 672})"
      ]
     },
     "execution_count": 193,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "Counter(y_train)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "HzAcgm1rACLu"
   },
   "source": [
    "## Bag of Words (2 балла)\n",
    "\n",
    "Реализуйте простой подсчет слов в тексте, в качестве токенизатора делите по пробелу, убрав перед этим все знаки пунктуации и приведя к нижнему регистру.\n",
    "\n",
    "После этого обучите простую логистическую модель, измерьте ее качество и сделайте выводы."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 194,
   "metadata": {
    "id": "lZsEPDLBACLu"
   },
   "outputs": [],
   "source": [
    "class BagOfWords:\n",
    "    def __init__(self, vocabulary_size: int = 1000):\n",
    "        \"\"\"Init Bag-of-Words instance\n",
    "\n",
    "        Args:\n",
    "            vocabulary_size: maximum number of tokens in vocabulary\n",
    "        \"\"\"\n",
    "        self._vocabulary_size = vocabulary_size\n",
    "        self._vocabulary: dict[str, int] = None\n",
    "\n",
    "    def _tokenize(self, sentence: str) -> list[str]:\n",
    "        sentence = re.sub(r'[^\\w\\s]', '', sentence.lower())\n",
    "        return sentence.split()\n",
    "        \n",
    "    def fit(self, sentences: list[str]):\n",
    "        \"\"\"Fit Bag-of-Words based on list of sentences\"\"\"\n",
    "        tokens = []\n",
    "        for sent in sentences:\n",
    "            tokens += self._tokenize(sent)\n",
    "        counter = Counter(tokens)\n",
    "        \n",
    "        common = counter.most_common(self._vocabulary_size)\n",
    "        self._vocabulary = {token: i for i, (token, _) in enumerate(common)}\n",
    "\n",
    "    def transform(self, sentences: list[str]) -> np.ndarray:\n",
    "        \"\"\"Vectorize texts using built vocabulary\n",
    "\n",
    "        Args:\n",
    "            sentences: list of sentences to vectorize\n",
    "\n",
    "        Return:\n",
    "            transformed texts, matrix of (n_sentences, vocab_size)\n",
    "        \"\"\"\n",
    "        if self._vocabulary is None:\n",
    "            raise RuntimeError(\"Fit before transforming!\")\n",
    "\n",
    "        vectors = np.zeros((len(sentences), self._vocabulary_size))\n",
    "        \n",
    "        for i, sentence in enumerate(sentences):\n",
    "            tokens = self._tokenize(sentence)\n",
    "            for token in tokens:\n",
    "                if token in self._vocabulary:\n",
    "                    token_idx = self._vocabulary[token]\n",
    "                    vectors[i, token_idx] += 1\n",
    "                    \n",
    "        return vectors\n",
    "\n",
    "    def fit_transform(self, sentences: list[str]) -> np.ndarray:\n",
    "        self.fit(sentences)\n",
    "        return self.transform(sentences)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 195,
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_bow_size(cls, params=None, metric=accuracy_score):\n",
    "    sizes = [10, 50, 100, 500, 1000, 1500, 2000, 2500, 3000, 3500, 4000, 5000]\n",
    "    best_score, best_size = 0, 0\n",
    "    params = params if params else dict()\n",
    "    for size in sizes:\n",
    "        \n",
    "        bow = cls(vocabulary_size=size, **params)\n",
    "        \n",
    "        X_train_bow = bow.fit_transform(X_train)\n",
    "        X_test_bow = bow.transform(X_test)\n",
    "        \n",
    "        log_reg = LogisticRegression()\n",
    "        log_reg.fit(X_train_bow, y_train)\n",
    "        y_pred = log_reg.predict(X_test_bow)\n",
    "        score = metric(y_test, y_pred)\n",
    "        \n",
    "        if score > best_score:\n",
    "            best_score, best_size = score, size\n",
    "    return best_size"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 196,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "3000"
      ]
     },
     "execution_count": 196,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "bow_best_size = get_bow_size(BagOfWords)\n",
    "bow_best_size"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 197,
   "metadata": {
    "id": "VuSY9FEARvW5"
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "((5016, 3000), (558, 3000))"
      ]
     },
     "execution_count": 197,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "bow = BagOfWords(vocabulary_size=bow_best_size)\n",
    "X_train_bow = bow.fit_transform(X_train)\n",
    "X_test_bow = bow.transform(X_test)\n",
    "\n",
    "X_train_bow.shape, X_test_bow.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 198,
   "metadata": {
    "id": "_cKMLYwMACLv"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "              precision    recall  f1-score   support\n",
      "\n",
      "         ham       0.99      1.00      1.00       483\n",
      "        spam       1.00      0.95      0.97        75\n",
      "\n",
      "    accuracy                           0.99       558\n",
      "   macro avg       1.00      0.97      0.98       558\n",
      "weighted avg       0.99      0.99      0.99       558\n",
      "\n"
     ]
    }
   ],
   "source": [
    "model = LogisticRegression()\n",
    "model.fit(X_train_bow, y_train)\n",
    "\n",
    "y_pred = model.predict(X_test_bow)\n",
    "print(classification_report(y_test, y_pred))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "KI089XBzACLv"
   },
   "source": [
    "## Обработка текста (1 балл)\n",
    "\n",
    "Добавьте на этапе токенизатора удаление стоп-слов и стемминг, для этого можно воспользоваться [`SnowballStemmer`](https://www.nltk.org/api/nltk.stem.SnowballStemmer.html) из библиотеки `nltk`.\n",
    "\n",
    "⚠️ `nltk` уже довольно устаревшая библиотека и скорее не рекомендуется ее использовать, однако в учебных целях более чем достаточно.\n",
    "\n",
    "Обучите логистическую регрессию, попробуйте по-разному комбинировать стемминг и удаление стоп-слов, сделайте выводы."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 199,
   "metadata": {
    "id": "yttB8gqVACLv"
   },
   "outputs": [],
   "source": [
    "class BagOfWordsStem(BagOfWords):\n",
    "    def __init__(\n",
    "        self,\n",
    "        vocabulary_size: int,\n",
    "        language: str = \"english\",\n",
    "        ignore_stopwords: bool = True,\n",
    "        remove_stopwords: bool = True,\n",
    "    ):\n",
    "        super().__init__(vocabulary_size)\n",
    "        if remove_stopwords and not ignore_stopwords:\n",
    "            raise ValueError(\"To remove stop-words they should be ignored by stemmer\")\n",
    "        self._stemmer = SnowballStemmer(language)\n",
    "        self._stopwords = set(stopwords.words(language))\n",
    "        self._remove_stopwords = remove_stopwords\n",
    "\n",
    "    def _tokenize(self, sentence: str) -> list[str]:\n",
    "        tokens = super()._tokenize(sentence)\n",
    "        result = []\n",
    "        \n",
    "        for token in tokens:\n",
    "            if self._remove_stopwords and token in self._stopwords:\n",
    "                continue\n",
    "            result.append(self._stemmer.stem(token))\n",
    "        return result"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 200,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "500"
      ]
     },
     "execution_count": 200,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "best_size = get_bow_size(BagOfWordsStem, {\"ignore_stopwords\": True, \"remove_stopwords\": True})\n",
    "best_size"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 201,
   "metadata": {
    "id": "7ROhMn0bACLv"
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "((5016, 500), (558, 500))"
      ]
     },
     "execution_count": 201,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "bow = BagOfWordsStem(vocabulary_size=best_size, ignore_stopwords=True, remove_stopwords=True)\n",
    "X_train_bow = bow.fit_transform(X_train)\n",
    "X_test_bow = bow.transform(X_test)\n",
    "\n",
    "X_train_bow.shape, X_test_bow.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 202,
   "metadata": {
    "id": "yUU-BcQ1ACLv"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "              precision    recall  f1-score   support\n",
      "\n",
      "         ham       0.99      1.00      0.99       483\n",
      "        spam       0.99      0.95      0.97        75\n",
      "\n",
      "    accuracy                           0.99       558\n",
      "   macro avg       0.99      0.97      0.98       558\n",
      "weighted avg       0.99      0.99      0.99       558\n",
      "\n"
     ]
    }
   ],
   "source": [
    "model = LogisticRegression()\n",
    "model.fit(X_train_bow, y_train)\n",
    "\n",
    "y_pred = model.predict(X_test_bow)\n",
    "print(classification_report(y_test, y_pred))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "2_IOo98iACLw"
   },
   "source": [
    "## TF-IDF (2 балла)\n",
    "\n",
    "Доработайте предыдущий класс до полноценного Tf-Idf, затем, аналогично, проведите эксперименты с логистической регрессией."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 208,
   "metadata": {
    "id": "6rzQ5sUOACLw"
   },
   "outputs": [],
   "source": [
    "class TFIDFVectorizer:\n",
    "    def __init__(\n",
    "        self,\n",
    "        vocabulary_size: int,\n",
    "        language: str = \"english\",\n",
    "        ignore_stopwords: bool = True,\n",
    "        remove_stopwords: bool = True,\n",
    "        use_idf: bool = False,\n",
    "    ):\n",
    "        self._vocabulary_size = vocabulary_size\n",
    "        self._vocabulary = None\n",
    "        self._idf = None\n",
    "        self._use_idf = use_idf\n",
    "\n",
    "        # Логику с токенизацией можно вынести в отдельный класс!\n",
    "        if remove_stopwords and not ignore_stopwords:\n",
    "            raise ValueError(\"To remove stop-words they should be ignored by stemmer\")\n",
    "        self._stemmer = SnowballStemmer(language)\n",
    "        self._stopwords = set(stopwords.words(language))\n",
    "        self._remove_stopwords = remove_stopwords\n",
    "        self._ignore_stopwords = ignore_stopwords\n",
    "\n",
    "    def _tokenize(self, sentence: str) -> list[str]:\n",
    "        sentence = re.sub(r'[^\\w\\s]', '', sentence.lower())\n",
    "        tokens = [token for token in sentence.split(' ') if token]\n",
    "        result = []\n",
    "        \n",
    "        for token in tokens:\n",
    "            is_stopword = token in self._stopwords\n",
    "\n",
    "            if self._remove_stopwords and is_stopword:\n",
    "                continue \n",
    "\n",
    "            if is_stopword and self._ignore_stopwords:\n",
    "                result.append(self._stemmer.stem(token))\n",
    "            else:\n",
    "                result.append(token)\n",
    "\n",
    "        return result\n",
    "        \n",
    "    def fit(self, sentences: list[str]):\n",
    "        \"\"\"Build vocabulary and compute IDF\"\"\"\n",
    "        term_freq = defaultdict(int)\n",
    "        doc_freq = defaultdict(int)\n",
    "        total_docs = len(sentences)\n",
    "        \n",
    "        for sentence in sentences:\n",
    "            tokens = self._tokenize(sentence)\n",
    "            unique_tokens = set(tokens)\n",
    "            \n",
    "            for token in unique_tokens:\n",
    "                doc_freq[token] += 1\n",
    "                \n",
    "            for token in tokens:\n",
    "                term_freq[token] += 1\n",
    "        \n",
    "        sorted_terms = sorted(doc_freq.items(), key=lambda x: x[1], reverse=True)\n",
    "        top_terms = [term for term, _ in sorted_terms[:self._vocabulary_size]]\n",
    "        \n",
    "        self._vocabulary = {term: idx for idx, term in enumerate(top_terms)}\n",
    "        \n",
    "        if self._use_idf:\n",
    "            self._idf = {}\n",
    "            for term in self._vocabulary:\n",
    "                self._idf[term] = np.log((total_docs + 1) / (doc_freq.get(term, 0) + 1) + 1)\n",
    "\n",
    "\n",
    "    def transform(self, sentences: list[str]) -> np.ndarray:\n",
    "        \"\"\"Transform sentences to TF-IDF vectors\"\"\"\n",
    "        vectors = np.zeros((len(sentences), len(self._vocabulary)))\n",
    "        \n",
    "        for i, sentence in enumerate(sentences):\n",
    "            tokens = self._tokenize(sentence)\n",
    "            total_terms = len(tokens)\n",
    "                \n",
    "            term_counts = defaultdict(int)\n",
    "            for token in tokens:\n",
    "                if token in self._vocabulary:\n",
    "                    term_counts[token] += 1\n",
    "            \n",
    "            for term, count in term_counts.items():\n",
    "                tf = count / total_terms\n",
    "                \n",
    "                if self._use_idf:\n",
    "                    tfidf = tf * self._idf[term]\n",
    "                else:\n",
    "                    tfidf = tf\n",
    "                \n",
    "                term_idx = self._vocabulary[term]\n",
    "                vectors[i, term_idx] = tfidf\n",
    "                \n",
    "        return vectors\n",
    "\n",
    "    def fit_transform(self, sentences: list[str]) -> np.ndarray:\n",
    "        self.fit(sentences)\n",
    "        return self.transform(sentences)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 209,
   "metadata": {
    "id": "iyVh0Cz2ACLw"
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "((5016, 4000), (558, 4000))"
      ]
     },
     "execution_count": 209,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "tfidf = TFIDFVectorizer(vocabulary_size=4000, remove_stopwords=True, use_idf=True)\n",
    "X_train_tfidf = tfidf.fit_transform(X_train)\n",
    "X_test_tfidf = tfidf.transform(X_test)\n",
    "\n",
    "X_train_tfidf.shape, X_test_tfidf.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 210,
   "metadata": {
    "id": "sg_Gac-jACLw"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "              precision    recall  f1-score   support\n",
      "\n",
      "         ham       0.98      1.00      0.99       483\n",
      "        spam       0.97      0.84      0.90        75\n",
      "\n",
      "    accuracy                           0.97       558\n",
      "   macro avg       0.97      0.92      0.94       558\n",
      "weighted avg       0.97      0.97      0.97       558\n",
      "\n"
     ]
    }
   ],
   "source": [
    "model = LogisticRegression()\n",
    "model.fit(X_train_tfidf, y_train)\n",
    "\n",
    "y_pred = model.predict(X_test_tfidf)\n",
    "print(classification_report(y_test, y_pred))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "oJe5cDxDRvW5"
   },
   "source": [
    "## NaiveBayes (5 баллов)\n",
    "\n",
    "Наивный байесовский классификатор — это простой и эффективный алгоритм машинного обучения, основанный на теореме Байеса с наивным предположением независимости признаков.\n",
    "\n",
    "### Формула Байеса\n",
    "\n",
    "$$\n",
    "P(A|B) = \\frac{P(B|A) \\cdot P(A)}{P(B)}\n",
    "$$\n",
    "\n",
    "В контексте классификации текста это значит: $P(класс | документ) \\propto P(класс) \\cdot P(документ | класс)$\n",
    "\n",
    "Почему \"наивность\"? Потому что предпологаем, что все слова независимы:\n",
    "\n",
    "$$\n",
    "P(w_1, w_2, \\dots | class) = P(w_1 | class) \\cdot P(w_2 | class) \\cdot \\dots\n",
    "$$\n",
    "\n",
    "### Классификация текста\n",
    "\n",
    "Таким образом, для классификации текста необходимо:\n",
    "\n",
    "1. Вычислить априорную вероятность класса: $P(class)$, доля документов с таким классом\n",
    "2. Вычислить правдоподобие: $P(text | class) = \\prod_{i=1}^n P(w_i | class)$\n",
    "\n",
    "_Примечание:_ $P(w_i | class)$ — это частота слова в данном классе относительно всех слов в классе, при этом зачастую добавляют сглаживание Лапласа в качестве регуляризатора\n",
    "$$\n",
    "P(w_i | class) = \\frac{\\text{частота слова в классе} + \\alpha}{\\text{сумма всех слов в классе} + \\alpha \\cdot |V|}\n",
    "$$\n",
    "\n",
    "После этого, необходимо выбрать наиболее вероятный класс для данного текста:\n",
    "\n",
    "$$\n",
    "class = \\arg \\max\\limits_{c} \\Big[ P(c) \\cdot P(text | c) \\Big] = \\arg \\max\\limits_{c} \\Big[ \\log P(c) + \\sum_{i=1}^n \\log P(w_i | c) \\Big]\n",
    "$$\n",
    "\n",
    "### Реализация\n",
    "\n",
    "`fit(X, y)` - оценивает параметры распределения `p(x|y)` для каждого `y`.\n",
    "\n",
    "`log_proba(X)` - для каждого элемента набора `X` считает логарифм вероятности отнести его к каждому классу."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 211,
   "metadata": {
    "id": "cQL-8wxwRvW5"
   },
   "outputs": [],
   "source": [
    "class NaiveBayes:\n",
    "\n",
    "    def __init__(self, alpha: float = 1.0):\n",
    "        \"\"\"\n",
    "        Args:\n",
    "            alpha: regularization coefficient\n",
    "        \"\"\"\n",
    "        self.alpha = alpha\n",
    "        self._classes = None  # [n classes]\n",
    "        self._vocab_size = None  # int\n",
    "        self._log_p_y = None  # [n classes]\n",
    "        self._log_p_x_y = None  # [n classes, vocab size]\n",
    "\n",
    "    def fit(self, features: np.ndarray, targets: list[str]):\n",
    "        \"\"\"Estimate p(x|y) and p(y) based on data\n",
    "\n",
    "        Args:\n",
    "            features, [n samples; vocab size]: input features\n",
    "            targets, [n samples]: targets\n",
    "        \"\"\"\n",
    "        targets = np.array(targets)\n",
    "        self._classes = np.unique(targets)\n",
    "        self._vocab_size = features.shape[1]\n",
    "        \n",
    "        class_counts = np.array([np.sum(targets == c) for c in self._classes])\n",
    "        self._log_p_y = np.log(class_counts / len(targets))\n",
    "        \n",
    "        self._log_p_x_y = np.zeros((len(self._classes), self._vocab_size))\n",
    "        \n",
    "        for i, c in enumerate(self._classes):\n",
    "            class_docs = features[targets == c]\n",
    "            word_counts = np.sum(class_docs, axis=0)\n",
    "            total_words = np.sum(word_counts)\n",
    "            self._log_p_x_y[i] = np.log((word_counts + self.alpha) / (total_words + self.alpha * self._vocab_size))\n",
    "\n",
    "    def predict(self, features: np.ndarray) -> np.ndarray:\n",
    "        \"\"\"Predict class for each sample\n",
    "\n",
    "        Args:\n",
    "            features, [n samples; vocab size]: feature to predict\n",
    "        Return:\n",
    "            classes, [n samples]: predicted class\n",
    "        \"\"\"\n",
    "        log_proba = self.log_proba(features)\n",
    "        return self._classes[np.argmax(log_proba, axis=1)]\n",
    "\n",
    "    def log_proba(self, features: np.ndarray) -> np.ndarray:\n",
    "        \"\"\"Calculate p(y|x) for each class and each sample\n",
    "\n",
    "        Args:\n",
    "            features, [n samples; vocab size]: feature to predict\n",
    "        Return:\n",
    "            classes, [n samples;  n classes]: log proba for each class\n",
    "        \"\"\"\n",
    "        if self._vocab_size is None:\n",
    "            raise RuntimeError(\"Fit classifier before predicting something\")\n",
    "        if features.shape[1] != self._vocab_size:\n",
    "            raise RuntimeError(\n",
    "                f\"Unexpected size of vocabulary, expected {self._vocab_size}, actual {features.shape[1]}\"\n",
    "            )\n",
    "        log_proba = np.zeros((features.shape[0], len(self._classes)))\n",
    "        \n",
    "        for i in range(len(self._classes)):\n",
    "            class_log_proba = features @ self._log_p_x_y[i]\n",
    "            log_proba[:, i] = self._log_p_y[i] + class_log_proba\n",
    "            \n",
    "        return log_proba"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 212,
   "metadata": {
    "id": "6YJEuNYRACLx"
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "((5016, 3000), (558, 3000))"
      ]
     },
     "execution_count": 212,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "bow = BagOfWordsStem(vocabulary_size=bow_best_size, remove_stopwords=True)\n",
    "X_train_bow = bow.fit_transform(X_train)\n",
    "X_test_bow = bow.transform(X_test)\n",
    "\n",
    "X_train_bow.shape, X_test_bow.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 213,
   "metadata": {
    "id": "spb2TAg1ACLx"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "              precision    recall  f1-score   support\n",
      "\n",
      "         ham       0.99      0.99      0.99       483\n",
      "        spam       0.92      0.96      0.94        75\n",
      "\n",
      "    accuracy                           0.98       558\n",
      "   macro avg       0.96      0.97      0.97       558\n",
      "weighted avg       0.98      0.98      0.98       558\n",
      "\n"
     ]
    }
   ],
   "source": [
    "model = NaiveBayes(alpha=1.0)\n",
    "model.fit(X_train_bow, y_train)\n",
    "\n",
    "y_pred = model.predict(X_test_bow)\n",
    "print(classification_report(y_test, y_pred))"
   ]
  }
 ],
 "metadata": {
  "colab": {
   "provenance": []
  },
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.7"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
